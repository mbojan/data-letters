---
title: "Extract and search for terms"
output: 
  github_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, cache=FALSE}
requireNamespace("jqr")
requireNamespace("jsonlite")
requireNamespace("XLConnect")
library(tidyverse)

knitr::opts_chunk$set(
  echo = TRUE
)
```


```{r functions, include=FALSE}
has_duplicate <- function(x)   x %in% unique(x[duplicated(x)])
```


# Read and filter the JSON


Read, filter and convert to tibble.

```{r read-filter, eval=FALSE}
d <- paste(readr::read_lines("network_data/network_poihnr_0_169221_pt_exist_af.json"), collapse="") %>%
  jqr::jq("[.network_data[] | {doc_type, doc_id, date, text}]") %>%
  fromJSON() %>%
  tibble::as_tibble(z)
```

Save

```{r save-csv, eval=FALSE}
readr::write_csv(d, "src.csv")
```






# Check

Read the CSV back

```{r reread-csv}
d <- readr::read_csv("src.csv")
```

Duplicates

```{r duplicated-all}
d %>%
  mutate_all(duplicated) %>%
  summarise_all(any)
```

- No duplicates on `doc_id`
- Duplicates on `text`?

Duplicate rows on `text`:

```{r duplicated-text}
d %>%
  filter(duplicated(text)) %>%
  arrange(text) %>%
  knitr::kable()
```




# Search

Load the terms

```{r read-terms}
fn <- "~/Desktop/SOCIAL CATEGORIES - Portuguese Empire (4).xlsx"
shnames <- readxl::excel_sheets(fn)
terms <- lapply(
  shnames,
  function(s) readxl::read_excel(
    fn, 
    sheet = s,
    col_names="term"
  )
) %>%
  set_names(shnames) %>%
  bind_rows(.id="sheet") %>%
  mutate(
    type = recode(
      sheet,
      "Population of color" = "race",
      "SUBALTERN GROUPS" = "subaltern",
      "WOMEN" = "women",
      "WOMENS NAMES" = "names"
    )
  )
  
```

```{r show-terms}
terms %>%
  filter(has_duplicate(term), !is.na(term)) %>%
  count(sheet, term) %>%
  spread(sheet, n) %>%
  knitr::kable()
```

Duplicated terms:

```{r duplicated-terms}
terms %>%
  filter(!is.na(term), !duplicated(term)) %>%
  count(type)
```

Add stem

```{r terms-stem}
uterms <- terms %>%
  filter(!is.na(term), !duplicated(term)) %>%
  mutate(
    # lcterm = tolower(term),
    stem = SnowballC::wordStem(term, language="portuguese")
  )
```


Get tokens from the documents


```{r tokenize}
td <- d %>%
  tidytext::unnest_tokens(word, text)
```


Join document tokens with search terms


```{r join-tokens-terms}
r <- td %>%
  semi_join(uterms, by=c("word"="term"))
```

Join full text and other variables

```{r join-full-text}
out <- r %>%
  distinct(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(
    word_id = seq(1, length(word))
  ) %>%
  spread(word_id, word) %>%
  left_join(d, by="doc_id") %>%
  select(doc_id, text, everything())
```

# Identify phrases to be ignored

```{r ignore-phrases}
igf <- c(
  "nau da Índia", 
  "guerra preta", 
  "vindo da Índia", 
  "para a Índia", 
  "castelhano", 
  "Francisco de Santa Bárbara Moura", 
  "branco", 
  "espanhol", 
  "soldado", 
  "João de Basto Moura", 
  "Rio Negro"
)
```

Look for ignored phrases in full text

```{r massage-ignored}
to_ignore <- parallel::mclapply(
  igf,
  function(p) grepl(p, out$text),
  mc.cores=2
) %>%
  set_names(igf) %>%
  as_tibble() %>%
  mutate(
    doc_id = out$doc_id
  ) %>%
  gather(igf, found, -doc_id) %>%
  group_by(doc_id) %>%
  summarise(
    any_found = any(found)
  )
```

Merge to the rest

```{r join-ignored}
out %>%
  left_join(to_ignore, by="doc_id") %>%
  filter(!any_found) -> out
```


# Save

Save to Excel-compatible CSV

```{r out-save-csv, eval=FALSE}
readr::write_excel_csv(out, path="search.csv")
```

Save to XLS directly

```{r save-xls}
XLConnect::writeWorksheetToFile(
  file = "search.xls",
  out %>%
    select(-any_found),
  sheet = "przeszukanie",
  clearSheets = TRUE
)
```

