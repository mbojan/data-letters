{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# This notebook is for the project Portugal Overseas Identity - Historical Network Research\n",
    "\n",
    "@Agatha, @Michal: feel free to add anything, like explanations about the project, comments, etc.\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1- To improve accuracy on existing entity types. Entity types that already exist in the spaCy xx_ent_wiki_sm (international language, including Portuguese) model can be found here: https://spacy.io/docs/usage/entity-recognition#entity-types\n",
    "UPDATE: spaCy package was updated to its version 2.0 with a Portuguese trained model. Nonetheless, for our purposes the model got a little bit less accurate. So re-training is even more important now.\n",
    "\n",
    "2- To extend the named entity recognizer. New entity types to be added to the model. Model will be saved as pt_poihnr. Entities to be added:\n",
    "- Role as `ROLE`\n",
    "- Type of document as `TYPE`\n",
    "- Afiiliation as 'AFF'\n",
    "- Date as 'DATE'\n",
    "\n",
    "\n",
    "3- After having a satisfactory NER model, to get senders and recipients from data as well as their roles. This is done using a mix of previous scripts using regular expressions and NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for training the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This functions reads 'portugal_documents.csv' (the original csv document of the project) and \n",
    "# uses only the 'full_text' column to create a new file with this information edited without square\n",
    "# breackets.\n",
    "\n",
    "def create_fulltext_file():\n",
    "    df = pd.read_csv('portugal_documents.csv') \n",
    "    full_text = df['full_text'].values         \n",
    "\n",
    "    with open('poi_full_text.csv', 'w') as outcsv:\n",
    "        writer = csv.writer(outcsv)\n",
    "        writer.writerow(['full_text'])\n",
    "        chars_to_remove = ['[', ']']\n",
    "        rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "        for i in range(len(full_text)):\n",
    "            line = re.sub(rx, '', full_text[i])\n",
    "            writer.writerow([line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_fulltext_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function reads the new 'poi_full_text.csv' created from function 'create_fulltext_file'. \n",
    "# It creates 4230 txt files, with only the abstract of the full_text (no technical information) picked\n",
    "# randomly.\n",
    "# These files feed maxQDA for tagging the entities, their types and their position in\n",
    "# the text. Moreover, they will serve as the training data for the NER model. \n",
    "\n",
    "def create_abstract_files()\n",
    "\n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    df = pd.read_csv('poi_full_text.csv')\n",
    "    full_text = df['full_text'].values\n",
    "\n",
    "    if not os.path.exists('train_poi_data_abstract_only'):\n",
    "        os.makedirs('train_poi_data_abstract_only')\n",
    "\n",
    "    random.seed(2001)\n",
    "    for i in range(4230):  #sample of 2.5% of the whole corpus\n",
    "    #for i in range(200):\n",
    "\n",
    "        a = random.randint(0,169221) #random sampling\n",
    "        text = full_text[a].decode('utf-8')\n",
    "\n",
    "        id_match = re.match(r'(\\d+)[-.]', full_text[a].decode('utf-8').lstrip(' '))\n",
    "\n",
    "        if id_match is not None:\n",
    "            g = open('train_poi_data_abstract_only/'+str(i)+'_'+id_match.group(1)+'.txt','w')\n",
    "        else:\n",
    "            g = open('train_poi_data_abstract_only/'+str(i)+'_noid.txt','w')\n",
    "\n",
    "        pattern = re.compile(r'.+?(?=\\.\\s\\b(An\\w{3,4}|AH|N|Obs))')\n",
    "        match = re.search(pattern,text)\n",
    "\n",
    "        if match:\n",
    "            g.write(match.group(0))\n",
    "            g.write('.')\n",
    "            g.write('\\n')\n",
    "        else:\n",
    "            g.write(text)\n",
    "\n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_abstract_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After maxQDA\n",
    "\n",
    "As a result of the work done with maxQDA, two '.rar' files were created with several excel files (corresponding to whoever used maxQDA for tagging the entities): AGATA MAXQDA.rar and OUTROS.rar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Renaming the class of entities for training the NER model later\n",
    "ner_folders = ['DATE', 'INSTITUTION', 'LOCALIZATION', 'PERSON', 'ROLE', 'TYPE', 'AFFILIATION']\n",
    "ners = ['DATE', 'ORG', 'LOC', 'PER', 'ROLE', 'TYPE', 'AFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First the files are rearranged in folders by entity class: AFFILIATION, DATE, INSTITUTION, \n",
    "# LOCALIZATION, PERSON, ROLE AND TYPE (type of the document).\n",
    "# Obs: some data cleaning was needed in this stage \n",
    "\n",
    "def rearrange_maxQDA_files(ner_folders):\n",
    "\n",
    "    for folder in sorted(os.listdir('maxqda_tagging/AGATA MAXQDA')):\n",
    "        for document in sorted(os.listdir('maxqda_tagging/AGATA MAXQDA/'+folder)):\n",
    "            for ner in ner_folders:\n",
    "                if ner in document:\n",
    "                    os.rename('maxqda_tagging/AGATA MAXQDA/'+folder+'/'+document, 'maxqda_tagging/'+ner+'/'+document)\n",
    "\n",
    "    for folder in sorted(os.listdir('maxqda_tagging/OUTROS')):\n",
    "        for document in sorted(os.listdir('maxqda_tagging/OUTROS/'+folder)):\n",
    "            for ner in ner_folders:\n",
    "                if ner in document:\n",
    "                    os.rename('maxqda_tagging/OUTROS/'+folder+'/'+document, 'maxqda_tagging/'+ner+'/'+document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rearrange_maxQDA_files(ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing training files with maxQDA output files\n",
    "# Obs: INPUT are the maxQDA files that NEED editing for corrections. \n",
    "# UPDATE 13-03-2018: AFFILIATION files OK!\n",
    "# UPDATE 24-04-2018: all entity classes OK! - Agata\n",
    "\n",
    "def write_training_files(ner_folders,ners):\n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    for i in range(len(ners)):\n",
    "\n",
    "        for excel in os.listdir('maxqda_tagging/'+ner_folders[i]):\n",
    "\n",
    "            print excel\n",
    "            df = pd.ExcelFile('maxqda_tagging/'+ner_folders[i]+'/'+excel)\n",
    "            df = df.parse(df.sheet_names[0], header=None,index_col=None)\n",
    "            filename = df.iloc[:,1].values[1:]\n",
    "            fragment = df.iloc[:,4].values[1:]\n",
    "\n",
    "            for j in range(len(filename)):\n",
    "\n",
    "                f = open('train_poi_data_abstract_only/'+filename[j]+'.txt','r')\n",
    "                text = f.readline().decode('utf-8')\n",
    "\n",
    "                if fragment[j][0] == ' ':\n",
    "                    fragment[j] = fragment[j][1:]\n",
    "\n",
    "                if fragment[j][-1] == ' ':\n",
    "                    fragment[j] = fragment[j][:-1]\n",
    "\n",
    "                match = re.search(fragment[j].decode('utf-8'), text)\n",
    "                #print match\n",
    "                if match is not None:\n",
    "\n",
    "                    if os.path.isfile('train_poi_data_maxqda/'+filename[j]+'.txt'):\n",
    "                        g = open('train_poi_data_maxqda/'+filename[j]+'.txt','a+')\n",
    "                        g.write('('+match.group(0)+'; '+str(match.span()[0])+'; '+str(match.span()[1])+'; '+ners[i]+')')\n",
    "                        g.write('\\n')\n",
    "\n",
    "                    else:\n",
    "                        g = open('train_poi_data_maxqda/'+filename[j]+'.txt','w')\n",
    "                        g.write(text)\n",
    "                        g.write('('+match.group(0)+'; '+str(match.span()[0])+'; '+str(match.span()[1])+'; '+ners[i]+')')\n",
    "                        g.write('\\n')\n",
    "\n",
    "                    g.close()\n",
    "\n",
    "                f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_training_files(ner_folders,ners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NER\n",
    "\n",
    "First, new entity classes to the already existing model 'pt' need to be added:\n",
    "- File: training_new_entities.py  \n",
    "\n",
    "New labels are in the list: LABEL = ['DATE','ROLE','TYPE','AFF']\n",
    "\n",
    "Second, new model with new entity classes:\n",
    "- File: training_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding sender/receiver and export json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "doc_id = pd.read_csv('portugal_documents.csv')['id'].values #documents id\n",
    "full_text = pd.read_csv('poi_full_text.csv')['full_text'] #documents text pre-processed\n",
    "\n",
    "#roles = pd.read_csv('roles_poi.csv')['roles'].values  #reads the list of roles manually \n",
    "                                                       #added to the csv file - NOT IN USE!\n",
    "\n",
    "#types = pd.read_csv('roles_poi.csv')['docs_avoided'].values #reads the list of doc to be avoided\n",
    "#types = types[:18]\n",
    "\n",
    "network_poi = []\n",
    "\n",
    "nlp = spacy.load('trained_models/pt_exist_af') #loads trained model\n",
    "\n",
    "#indicis for the pliting the files\n",
    "start = 0\n",
    "stop = len(doc_id)\n",
    "\n",
    "# testing first with few documents\n",
    "for i in range(start,stop,1):\n",
    "    \n",
    "    text = full_text[i].decode('utf-8')\n",
    "    \n",
    "    pattern = re.compile(r'.+?(?=\\.\\s\\b(An\\w{3,4}|AH|N|Obs))')\n",
    "    match = re.search(pattern,text)\n",
    "\n",
    "    if match:\n",
    "        text = match.group(0)\n",
    "    else:\n",
    "        text = text\n",
    "    \n",
    "    #matching doc type to filter types not wanted in the network - NOT IN USE NOW (CHECK WITH AGATA)\n",
    "    doc_pattern = re.compile(r'\\s?\\b([A-ZÃÁÀÂÇÉÊÍÕÓÔÚÜ]{2,}\\s?)+\\b')\n",
    "    doc_type = re.search(doc_pattern, text)\n",
    "    if doc_type:\n",
    "        doc_type = doc_type.group(0).replace(' ','')\n",
    "    else:\n",
    "        doc_type = 'NA'\n",
    "    \n",
    "    # variable to continue loop avoiding certain types of document\n",
    "    avoid = 0\n",
    "    #for doc in types:\n",
    "        #if doc.decode('utf-8') == doc_type:\n",
    "            #avoid = 1\n",
    "    \n",
    "    if avoid:\n",
    "        continue\n",
    "            \n",
    "    else:\n",
    "        #doc = nlp(text)\n",
    "        doc_dict = {'doc_id':doc_id[i], 'doc_type':doc_type, 'text':text, \\\n",
    "                    'sender':{'names':[],'roles':[], 'aff':[], 'org':[]}, \\\n",
    "                    'recipient':{'names':[],'roles':[], 'aff':[], 'org':[]}, \\\n",
    "                    'others':[]}\n",
    "        \n",
    "        s_pattern = re.compile(doc_type + r'.*?\\b(do|pelo|de|dos|da)\\b\\s(.*?)\\s+(à|a|ao|aos|para)\\s?(.+)(\\b.+ndo\\b|\\b\\w+ar\\b)?')\n",
    "        sender = re.search(s_pattern, text)\n",
    "        \n",
    "        r_pattern = re.compile(doc_type + r'.*?\\b(do|pelo|de|dos|da)\\b\\s(.*?)\\s+(à|a|ao|aos|para)\\s(.+)(\\b.+ndo\\b|\\ba\\b\\s\\w+r\\b|sobre|em que)')\n",
    "        recipient = re.search(r_pattern, text)\n",
    "        \n",
    "        #treating sender and recipients (NERs involved: PER, ROLE, AFF, ORG)\n",
    "        if sender:\n",
    "            doc = nlp(sender.group(2))\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PER':\n",
    "                    doc_dict['sender']['names'].append(ent.text)\n",
    "                if ent.label_ == 'ROLE':\n",
    "                    doc_dict['sender']['roles'].append(ent.text)\n",
    "                if ent.label_ == 'AFF':\n",
    "                    doc_dict['sender']['aff'].append(ent.text)\n",
    "                if ent.label_ == 'ORG':\n",
    "                    doc_dict['sender']['org'].append(ent.text)\n",
    "            text = text.replace(sender.group(2),'')\n",
    "                    \n",
    "        if recipient:\n",
    "            doc = nlp(recipient.group(4))\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PER':\n",
    "                    doc_dict['recipient']['names'].append(ent.text)\n",
    "                if ent.label_ == 'ROLE':\n",
    "                    doc_dict['recipient']['roles'].append(ent.text)\n",
    "                if ent.label_ == 'AFF':\n",
    "                    doc_dict['recipient']['aff'].append(ent.text)\n",
    "                if ent.label_ == 'ORG':\n",
    "                    doc_dict['recipient']['org'].append(ent.text)\n",
    "            text = text.replace(recipient.group(4),'')\n",
    "        \n",
    "        #treating type of document (NER involved: TYPE)\n",
    "        text = text.replace(doc_type,'')\n",
    "        \n",
    "        #treating date of document (NER involved: DATE)\n",
    "        date_match = re.search(r'\\d{4},(.*),\\ \\d{1,2}', text.decode('utf-8')) #search for date: YYYY, month, DD\n",
    "        if date_match is not None:\n",
    "            doc_dict['date'] = date_match.group(0)\n",
    "            text = text.replace(date_match.group(0),'')\n",
    "        else:\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'DATE':\n",
    "                    doc_dict['date'] = ent.text\n",
    "                    text = text.replace(ent.text,'')\n",
    "                    \n",
    "        #treating other NERs\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            doc_dict['others'].append(ent.text)\n",
    "            \n",
    "            \n",
    "    network_poi.append(doc_dict)            \n",
    "                    \n",
    "\n",
    "### In .json format    \n",
    "output = {\n",
    "    'network_data': network_poi\n",
    "}\n",
    "\n",
    "### Saves file\n",
    "with open('network_data/network_poihnr_'+str(stop)+'_pt_exist_af.json', 'wb') as jsonfile:\n",
    "    json.dump(output,jsonfile,ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old scripts: can be deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV file and creating JSON/TXT files with the data to train the model\n",
    "\n",
    "Update 13-03-2018: This was the fisrt attemp to create training files for the NER model. It was not used in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('poi_full_text.csv') #reads all the data and store in a dataframe\n",
    "full_text = df['full_text'].values    #gets only the 'full_tex' column data in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### DUMPING IN A JSON FILE #####\n",
    "def dump_json(full_text):\n",
    "    \n",
    "    #Identified mixed labels\n",
    "    #DATE as PER\n",
    "    months = ['Janeiro', 'Fevereiro', 'Março', 'Abril', 'Maio', 'Junho', 'Julho', \\\n",
    "              'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']    \n",
    "    \n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    nlp = xx_ent_wiki_sm.load() #loads NLP model for 'international' languages\n",
    "    train_poi = []\n",
    "    \n",
    "    roles = pd.read_csv('roles_poi.csv')['roles'].values  #reads the list of roles manually added to the csv file\n",
    "\n",
    "    random.seed(2001)\n",
    "    for i in range(4230):  #sample of 2.5% of the whole corpus\n",
    "        a = random.randint(0,169221) #random sampling\n",
    "        doc = nlp(full_text[a].decode('utf-8'))\n",
    "        doc_dict = {'text':full_text[a].decode('utf-8'), 'entity':0}\n",
    "\n",
    "        date_match = re.search(r'\\d{4},(.*),\\ \\d{1,2}', full_text[a].decode('utf-8')) #search for date: YYYY, month, DD\n",
    "\n",
    "        #iterate through all the entities found\n",
    "        entity_list = []\n",
    "        for ent in doc.ents:\n",
    "\n",
    "            entity_dict = {'entity_text': ent.text.decode('utf-8'), 'start': ent.start_char, \\\n",
    "                           'end': ent.end_char, 'label': ent.label_.encode('utf-8')}\n",
    "\n",
    "            #correcting DATE entity for the training file\n",
    "            if ent.text.decode('utf-8') in months:\n",
    "                if date_match is not None:\n",
    "                    entity_dict = {'entity_text': date_match.group(0), 'start': date_match.span()[0], \\\n",
    "                           'end': date_match.span()[1], 'label': 'DATE'}\n",
    "                else:  \n",
    "                    entity_dict['label'] = 'DATE'\n",
    "\n",
    "            entity_list.append(entity_dict)\n",
    "            \n",
    "        ### Adds ROLE entities based on roles_poi.csv file to the entity list\n",
    "        for role in roles:\n",
    "            role_match = re.search(role.decode('utf-8'), full_text[a].decode('utf-8'))\n",
    "            if role_match is not None:\n",
    "                dict_role = {'entity_text': role_match.group(0), 'start': role_match.span()[0], \\\n",
    "                               'end': role_match.span()[1], 'label': 'ROLE'}\n",
    "                entity_list.append(dict_role)\n",
    "\n",
    "        ### Gets the list of sentences       \n",
    "        doc_dict['entity'] = entity_list                                 \n",
    "        train_poi.append(doc_dict)\n",
    "\n",
    "    ### In .json format    \n",
    "    output = {\n",
    "        'train_data': train_poi\n",
    "    }\n",
    "\n",
    "    ### Saves file\n",
    "    with open('train_poi_data.json', 'wb') as jsonfile:\n",
    "        json.dump(output,jsonfile,ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### WRITING THE JSON FILE #####\n",
    "dump_json(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DUMPING IN A TXT FILE #####\n",
    "def dump_txt(full_text):\n",
    "\n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    #Identified mixed labels\n",
    "    #DATE as PER\n",
    "    months = ['Janeiro', 'Fevereiro', 'Março', 'Abril', 'Maio', 'Junho', 'Julho', \\\n",
    "              'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']\n",
    "\n",
    "    nlp = xx_ent_wiki_sm.load()\n",
    "\n",
    "    roles = pd.read_csv('roles_poi.csv')['roles'].values  #reads the list of roles manually added to the csv file\n",
    "\n",
    "    g = open('train_poi_data.txt','w')\n",
    "\n",
    "    random.seed(2001)\n",
    "    for i in range(4230):  #sample of 2.5% of the whole corpus\n",
    "    #for i in range(200):\n",
    "        a = random.randint(0,169221) #random sampling\n",
    "        doc = nlp(full_text[a].decode('utf-8'))\n",
    "\n",
    "        date_match = re.search(r'\\d{4},(.*),\\ \\d{1,2}', full_text[a].decode('utf-8')) #search for date: YYYY, month, DD\n",
    "\n",
    "        g.write('\\n')\n",
    "\n",
    "        for ent in doc.ents:\n",
    "\n",
    "            #correcting DATE entity for the training file\n",
    "            if ent.text in months:\n",
    "                if date_match is not None:\n",
    "                    g.write('('+date_match.group(0)+'; '+str(date_match.span()[0])+'; '+str(date_match.span()[1])+'; DATE)')\n",
    "                    g.write('\\n')\n",
    "\n",
    "            else:   \n",
    "                g.write('('+ent.text+'; '+str(ent.start_char)+'; '+str(ent.end_char)+'; '+ent.label_.encode('utf-8')+')')\n",
    "                g.write('\\n')\n",
    "\n",
    "        g.write('\\n')\n",
    "        g.write('\\n')\n",
    "\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### WRITING THE TXT FILE #####\n",
    "dump_txt(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DUMPING IN A FOLDER INDIVIDUAL TXT FILEs #####\n",
    "def dump_txt_individual(full_text):\n",
    "    \n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    #Identified mixed labels\n",
    "    #DATE as PER\n",
    "    months = ['Janeiro', 'Fevereiro', 'Março', 'Abril', 'Maio', 'Junho', 'Julho', \\\n",
    "              'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']\n",
    "\n",
    "    nlp = xx_ent_wiki_sm.load()\n",
    "\n",
    "    roles = pd.read_csv('roles_poi.csv')['roles'].values  #reads the list of roles manually added to the csv file\n",
    "    \n",
    "    if not os.path.exists('train_poi_data'):\n",
    "        os.makedirs('train_poi_data')\n",
    "\n",
    "    random.seed(2001)\n",
    "    for i in range(4230):  #sample of 2.5% of the whole corpus\n",
    "        a = random.randint(0,169221) #random sampling\n",
    "        doc = nlp(full_text[a].decode('utf-8'))\n",
    "\n",
    "        date_match = re.search(r'\\d{4},(.*),\\ \\d{1,2}', full_text[a].decode('utf-8')) #search for date: YYYY, month, DD\n",
    "        id_match = re.match(r'(\\d+)[-.]', full_text[a].decode('utf-8').lstrip(' '))\n",
    "\n",
    "        if id_match is not None:\n",
    "            g = open('train_poi_data/'+str(i)+'_'+id_match.group(1)+'.txt','w')\n",
    "        else:\n",
    "            g = open('train_poi_data/'+str(i)+'_noid.txt','w')\n",
    "\n",
    "        g.write('\\n')\n",
    "\n",
    "        for ent in doc.ents:\n",
    "\n",
    "            #correcting DATE entity for the training file\n",
    "            if ent.text in months:\n",
    "                if date_match is not None:\n",
    "                    g.write('('+date_match.group(0)+'; '+str(date_match.span()[0])+'; '+str(date_match.span()[1])+'; DATE)')\n",
    "                    g.write('\\n')\n",
    "\n",
    "            else:   \n",
    "                g.write('('+ent.text+'; '+str(ent.start_char)+'; '+str(ent.end_char)+'; '+ent.label_.encode('utf-8')+')')\n",
    "                g.write('\\n')\n",
    "\n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CREATING THE FOLDER AND WRITING INDIVIDUAL TXT FILES IN THERE #####\n",
    "dump_txt_individual(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Area!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
